{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50f730b",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ee617",
   "metadata": {},
   "source": [
    "## Q1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "The Hadoop ecosystem is a suite of tools designed to handle large-scale data processing and storage. The core components are:\n",
    "\n",
    "HDFS (Hadoop Distributed File System): A distributed file system that stores data across multiple machines in large blocks (default 128 MB), ensuring high availability and fault tolerance through replication.\n",
    "\n",
    "MapReduce: A programming model for processing large datasets in parallel by dividing tasks into Map and Reduce functions. It processes data in a distributed manner across multiple nodes.\n",
    "\n",
    "YARN (Yet Another Resource Negotiator): A cluster management framework that allocates resources and schedules jobs across the nodes. YARN provides a more flexible and efficient resource management layer compared to the older Hadoop 1.x architecture.\n",
    "\n",
    "## Q2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and how they contribute to data reliability and fault tolerance.\n",
    "HDFS is the primary storage system in Hadoop, designed to store very large datasets. It splits data into blocks (default 128 MB) and distributes them across different DataNodes in the cluster.\n",
    "\n",
    "NameNode: The master node that manages metadata (file names, permissions, locations of blocks). It does not store actual data but coordinates access to the files.\n",
    "\n",
    "DataNodes: Worker nodes that store the actual data blocks. DataNodes periodically report back to the NameNode with the status of their stored blocks.\n",
    "\n",
    "Blocks: Files in HDFS are broken into fixed-size blocks (typically 128 MB or 256 MB) that are replicated across multiple DataNodes for fault tolerance.\n",
    "\n",
    "HDFS achieves fault tolerance through replication (default is 3 copies of each block), ensuring data availability even if a node fails.\n",
    "\n",
    "## Q3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing large datasets.\n",
    "MapReduce processes data in two main phases:\n",
    "\n",
    "Map Phase:\n",
    "\n",
    "The input dataset is split into key-value pairs.\n",
    "Each mapper processes these pairs in parallel to produce intermediate key-value pairs.\n",
    "Example: In a word count problem, the input text is split, and the mapper outputs pairs like (word, 1).\n",
    "\n",
    "Shuffle and Sort:\n",
    "\n",
    "Intermediate key-value pairs are grouped by key, shuffled, and sorted before being passed to the reducers.\n",
    "Reduce Phase:\n",
    "\n",
    "The reducer processes each group of intermediate values to generate the final output.\n",
    "Example: In the word count, the reducer sums up the occurrences of each word.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Scalability: Can handle petabytes of data.\n",
    "Fault tolerance: Automatic recovery from hardware failures.\n",
    "Limitations:\n",
    "\n",
    "Latency: High overhead in managing multiple phases.\n",
    "No iterative processing: Not efficient for machine learning algorithms, which require multiple passes over the data.\n",
    "## Q4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications. Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "YARN is the resource management layer in Hadoop that separates resource management from job scheduling. YARN manages resources in two components:\n",
    "\n",
    "ResourceManager (RM): Allocates resources to different applications.\n",
    "NodeManager (NM): Monitors resources and execution on individual nodes.\n",
    "Hadoop 1.x had a fixed MapReduce job tracker, which managed both resource allocation and task scheduling, leading to scalability issues.\n",
    "\n",
    "Benefits of YARN:\n",
    "\n",
    "Better resource utilization: Allows multiple applications to run in parallel on the same cluster.\n",
    "Scalability: Can scale to thousands of nodes.\n",
    "Supports diverse workloads: Not limited to MapReduce (can run Spark, Tez, etc.).\n",
    "## Q5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig, and Spark. Describe the use cases and differences between these components. Choose one component and explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "HBase: A NoSQL database that provides real-time read/write access to large datasets stored in HDFS.\n",
    "\n",
    "Hive: A data warehouse infrastructure that allows querying and managing large datasets using SQL-like syntax (HiveQL). Best suited for batch processing.\n",
    "\n",
    "Pig: A high-level scripting language for processing large datasets. It provides data flow execution via Pig Latin scripts.\n",
    "\n",
    "Spark: A fast, in-memory data processing engine that supports real-time stream processing, machine learning, and iterative algorithms.\n",
    "\n",
    "Hive Example: Hive can be integrated into a Hadoop ecosystem to perform SQL-based queries on structured data stored in HDFS. This allows analysts familiar with SQL to work with large datasets using a declarative approach.\n",
    "\n",
    "## Q6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome some of the limitations of MapReduce for big data processing tasks?\n",
    "Key differences between Apache Spark and Hadoop MapReduce:\n",
    "\n",
    "In-memory processing: Spark processes data in-memory, leading to faster computations compared to the disk-based MapReduce.\n",
    "Ease of use: Spark provides APIs for Java, Scala, Python, and R, while MapReduce involves writing complex Java code.\n",
    "Real-time processing: Spark supports real-time stream processing (e.g., Spark Streaming), whereas MapReduce is batch-oriented.\n",
    "Iterative algorithms: Spark is efficient for iterative algorithms used in machine learning, while MapReduce requires multiple passes over the data.\n",
    "Spark overcomes MapReduceâ€™s limitations by reducing disk I/O and providing faster performance for iterative and real-time processing.\n",
    "\n",
    "## Q7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word, and returns the top 10 most frequent words. Explain the key components and steps involved in this application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bf1a2",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark context\n",
    "sc = SparkContext(\"local\", \"WordCountApp\")\n",
    "\n",
    "# Read text file\n",
    "text_file = sc.textFile(\"path/to/textfile.txt\")\n",
    "\n",
    "# Word count logic\n",
    "word_counts = text_file.flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get top 10 most frequent words\n",
    "top_10_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Print the results\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc550ff6",
   "metadata": {},
   "source": [
    "Key components:\n",
    "\n",
    "flatMap: Splits each line into words.\n",
    "map: Maps each word to a pair (word, 1).\n",
    "reduceByKey: Aggregates word counts.\n",
    "takeOrdered: Retrieves the top 10 frequent words.\n",
    "## Q8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your choice:\n",
    "a. Filter: rdd.filter(lambda row: row['value'] > 100) b. Map: rdd.map(lambda row: (row['column1'], row['column2'] * 2)) c. Reduce: rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "## Q9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfa1fb",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Load CSV dataset\n",
    "df = spark.read.csv(\"path/to/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select specific columns\n",
    "df.select(\"column1\", \"column2\").show()\n",
    "\n",
    "# b. Filter rows\n",
    "df.filter(df[\"column1\"] > 100).show()\n",
    "\n",
    "# c. Group by and aggregate\n",
    "df.groupBy(\"column3\").agg({\"column2\": \"sum\"}).show()\n",
    "\n",
    "# d. Join two DataFrames\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c0932",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d149bbc",
   "metadata": {},
   "source": [
    "## Q11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in the context of big data and real-time data processing?\n",
    "Apache Kafka is a distributed event streaming platform that handles high-throughput real-time data feeds. Kafka solves the problem of ingesting, processing, and storing large streams of data by providing:\n",
    "\n",
    "Publish/subscribe messaging model for real-time processing.\n",
    "Decoupling of producers and consumers to scale applications.\n",
    "**Fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc959d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
